# Search & Filtering

The Visual Layer SDK provides powerful search and filtering capabilities to help you find specific images in your datasets based on various criteria.

## Overview

The SDK supports multiple search methods:

- **Label-based search** - Find images by object/classification labels
- **Caption-based search** - Semantic search using AI-generated captions
- **Visual similarity search** - Find visually similar images
- **Issue-based search** - Filter by quality issues (blur, brightness, etc.)
- **VQL queries** - Advanced queries using Visual Query Language
- **Asynchronous search** - Handle large datasets efficiently

## Label-Based Search

Search for images containing specific labels or classifications.

### Basic Label Search

```python
# Search for images with specific labels
results_df = dataset.search_by_labels(["cat"])
print(f"Found {len(results_df)} images with cats")

# Multiple labels
leaf_results = dataset.search_by_labels(["angular_leaf_spot", "bean_rust"])
print(f"Found {len(leaf_results)} images with leaf diseases")
```

### Asynchronous Label Search

For large datasets, use asynchronous search to avoid timeouts:

```python
# Start async search and wait for completion
results_df = dataset.search_by_labels_async_to_dataframe(
    labels=["bean_rust", "angular_leaf_spot"],
    poll_interval=10,  # Check every 10 seconds
    timeout=300        # Maximum 5 minutes
)
print(f"Async search found {len(results_df)} images")
```

### Manual Async Search Control

For more control over the async process:

```python
# Start the search task
task_response = dataset.search_by_labels_async(["cat", "dog"])
task_id = task_response.get("id")
download_uri = task_response.get("download_uri")

# Check if ready immediately
if download_uri:
    results_df = dataset.process_export_download_to_dataframe(download_uri)
else:
    print(f"Task {task_id} is still processing...")
    # Poll manually or wait
```

## Caption-Based Search

Search using natural language descriptions generated by AI.

### Semantic Caption Search

```python
# Search for images with people holding objects
results_df = dataset.search_by_captions("person holding leaf")
print(f"Found {len(results_df)} images matching the description")

# Search with different similarity threshold
results_df = dataset.search_by_captions(
    caption_text="blue nail", 
    similarity_threshold=0.75  # Lower threshold = more results
)
```

### Asynchronous Caption Search

```python
# Start async caption search
task_response = dataset.search_by_captions_async("person with glasses")
task_id = task_response.get("id")
print(f"Started caption search task: {task_id}")
```

## Advanced Search Methods

*Note: These methods may be available depending on your dataset and Visual Layer configuration.*

### Visual Similarity Search

Find images visually similar to a reference image:

```python
# Search by uploading a local image
similar_images_df = dataset.search_by_visual_similarity_to_dataframe(
    image_path="path/to/reference_image.jpg"
)
print(f"Found {len(similar_images_df)} visually similar images")
```

### Issue-Based Search

Filter images by quality issues:

```python
# Find blurry images
blur_df = dataset.search_by_issues_to_dataframe(issue_type='blur')
print(f"Found {len(blur_df)} blurry images")

# Other issue types: 'brightness', 'contrast', 'noise', etc.
bright_df = dataset.search_by_issues_to_dataframe(issue_type='brightness')
```

### VQL (Visual Query Language)

Use structured queries for complex filtering:

```python
# Search for cats using VQL
cats_df = dataset.search_by_vql([
    {'id': 'label_filter', 'labels': {'op': 'one_of', 'value': ['cat']}}
])

# Complex VQL query example
complex_results = dataset.search_by_vql([
    {'id': 'label_filter', 'labels': {'op': 'one_of', 'value': ['dog']}},
    {'id': 'issue_filter', 'issues': {'op': 'none_of', 'value': ['blur']}}
])
```

## Working with Search Results

### Understanding Result DataFrames

Search results contain rich metadata:

```python
results_df = dataset.search_by_labels(["cat"])

# Common columns in results
print("Available columns:", results_df.columns.tolist())

# Typical columns include:
# - media_id: Unique image identifier
# - image_uri: URL to access the image
# - labels: Comma-separated labels
# - captions: AI-generated descriptions
# - cluster_id: Similarity cluster identifier
# - metadata: Additional image information
```

### Accessing Images

```python
# Get image information and URLs
for idx, row in results_df.head().iterrows():
    media_id = row['media_id']
    image_info = dataset.get_image_info(media_id)
    image_url = image_info['image_uri']
    print(f"Image {media_id}: {image_url}")
```

### Processing Results

```python
# Filter results further
high_confidence = results_df[results_df['confidence'] > 0.8]

# Group by labels
label_counts = results_df['labels'].value_counts()
print("Label distribution:", label_counts.head())

# Save results
results_df.to_csv("search_results.csv", index=False)
```

## Search Performance Tips

### 1. Use Appropriate Search Methods

- **Small datasets**: Use synchronous methods (`search_by_labels`, `search_by_captions`)
- **Large datasets**: Use asynchronous methods (`search_by_labels_async_to_dataframe`)
- **Complex queries**: Use VQL for structured filtering

### 2. Optimize Search Parameters

```python
# For caption search, adjust similarity threshold
results = dataset.search_by_captions(
    "dog running",
    similarity_threshold=0.7  # Lower = more results, higher = more precise
)

# For async search, tune polling parameters
results = dataset.search_by_labels_async_to_dataframe(
    labels=["cat"],
    poll_interval=15,  # Longer intervals for large datasets
    timeout=600        # Extend timeout for complex searches
)
```

### 3. Handle Large Result Sets

```python
# Process results in chunks
def process_large_results(results_df, chunk_size=1000):
    for start in range(0, len(results_df), chunk_size):
        chunk = results_df.iloc[start:start + chunk_size]
        # Process chunk
        print(f"Processing chunk {start//chunk_size + 1}: {len(chunk)} items")
        # Your processing logic here

# Use for very large result sets
if len(results_df) > 10000:
    process_large_results(results_df)
```

## Search Result Metadata

Understanding the rich metadata in search results:

### Processed Export Results

When using async methods with `process_export_download_to_dataframe()`, results include:

```python
results_df = dataset.search_by_labels_async_to_dataframe(['cat'])

# Rich metadata columns:
print("Captions:", results_df['captions'].iloc[0])           # AI descriptions
print("Image labels:", results_df['image_labels'].iloc[0])   # Classifications  
print("Object labels:", results_df['object_labels'].iloc[0]) # Object detection
print("Issues:", results_df['issues'].iloc[0])               # Quality problems
```

### Metadata Format

- **Captions**: Semicolon-separated AI-generated descriptions
- **Image labels**: Format: `"category(source); category(source)"`
- **Object labels**: Format: `"category[x1,y1,x2,y2]; category[x1,y1,x2,y2]"`
- **Issues**: Format: `"issue_type:description(confidence); ..."`

## Next Steps

- Check {doc}`API Reference <api-reference>` for detailed method signatures
- Learn about {doc}`Logging <logging>` to debug search operations